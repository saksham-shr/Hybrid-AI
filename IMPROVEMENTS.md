1. Executive Summary
The initial goal of this challenge was to debug and complete a partially working hybrid AI assistant. However, as I began fixing the system, it became clear that several components, especially the reasoning and embedding layers, were either outdated, dependent on external APIs, or unstable under real-world constraints.
Because of this, the work evolved from simple debugging into a complete re-architecture of the system. The process involved identifying broken components, addressing hardware and compatibility issues, and redesigning how data retrieval, embeddings, and model inference were handled.
The main constraints I faced were:
External API limitations, including paywall and quota restrictions.
Hardware compatibility issues with Apple Silicon (M1 Pro).
Model stability problems under long or complex input contexts.
The final system is now a fully self-hosted Retrieval-Augmented Generation (RAG) pipeline that runs end-to-end on local hardware. It uses a quantized Llama 3 model, GPU acceleration via Metal, and an asynchronous data retrieval and caching mechanism. The restructured architecture is faster, more reliable, and independent of paid APIs.
This report explains the debugging process, the architectural redesigns, and the reasoning behind each major decision that led to a successful, fully local AI system.

2. Debugging and Re-Architecture
The development process moved in three main phases, starting with system restoration, moving to a self-hosted model setup, and ending with performance optimization on limited hardware.
Phase I: System Restoration & Initial Blockers
The first task was to make the provided code executable and confirm the baseline functionality.
Problem 1:
The system failed to start due to multiple ModuleNotFoundError exceptions.
Diagnosis:
The provided virtual environment (venv) was not portable and did not include all dependencies required by the project.
Solution:
I rebuilt the environment from scratch using a clean Python environment and reinstalled the necessary libraries. Once this was done, the scripts began to run, confirming that the first stage was successful.
Problem 2:
The pinecone_upload.py script failed to upload data with a 404 Not Found error.
Diagnosis:
The Pinecone environment variable was hardcoded to an incorrect region (us-east1-gcp), while my account was on us-east-1(AWS).
Solution:
I updated the environment string and modified the create_index call to explicitly set the cloud parameter to "aws". This fixed the data ingestion process, allowing embeddings to upload successfully to Pinecone.
At this stage, the system was functional but limited by external dependencies for embeddings and LLM calls.
Phase II: The First Major Pivot – A Fully Self-Hosted System
Once the system could run, the main obstacle appeared during runtime.
Problem:
Every call to OpenAI’s API failed with a 429: insufficient_quota error.
Diagnosis:
This was due to a payment processing delay outside my control. Since the project deadline was fixed, waiting for quota renewal was not an option.
Strategic Pivot:
After consulting with the challenge organizers, I was told I could use any available LLM. This gave me the flexibility to move away from the API-based design and build a fully self-hosted alternative.
Embedding Solution:
I replaced the OpenAI embedding API with the local all-MiniLM-L6-v2 model from sentence-transformers. This required reconfiguring the Pinecone index to match the smaller 384-dimensional vectors generated by the new model.
Reasoning (Attempt 1):
For the reasoning layer, I initially replaced the OpenAI chat endpoint with google/flan-t5-base, a locally runnable model. While it did run, the quality of the responses was poor and lacked contextual understanding.
This made it clear that using smaller, instruction-tuned local models required deeper architectural changes to maintain the reasoning quality expected from the original system.
Phase III: Overcoming Hardware & Model Limitations
The switch to local models introduced new problems related to hardware limitations and model efficiency.
Problem:
Running flan-t5-base on my M1 Pro Mac caused a system-level bus error, abruptly terminating execution.
Diagnosis:
This was traced to a memory overflow and partial incompatibility between the model’s CPU execution and Apple’s ARM-based architecture.
Solution (Attempt 1):
I modified the code to use PyTorch’s Metal Performance Shaders (MPS) backend to run the model on the M1 GPU instead of the CPU. This improved speed but did not fully solve the issue, the model still crashed when the input context was too large.
Solution (Attempt 2):
I downgraded to the smaller flan-t5-small model, which resolved the memory issue but drastically reduced output quality. The model produced incomplete or repetitive responses, which failed to meet the challenge’s functional requirements.
Final Pivot:
After testing multiple options, I switched to the Meta-Llama-3-8B-Instruct model in a 4-bit quantized GGUF format, loaded via llama-cpp-python.
This combination provided:
Compatibility with Apple Silicon GPUs out of the box,
Significantly reduced memory usage through quantization,
High-quality instruction-following performance.
With this change, the system became both stable and efficient, capable of running large-scale inference entirely offline.

3. Final Architecture & Advanced Features
After stabilizing the model, I focused on enhancing the architecture for better reasoning quality, reliability, and speed. The final system integrates retrieval, reasoning, and optimization components seamlessly.
a. Chain-of-Thought Reasoning: “Summarize-then-Answer” Pipeline
To improve reasoning quality, I added a two-step reasoning pipeline:
Summarization Phase: The system first retrieves relevant data from Pinecone and Neo4j and asks the model to summarize the material concisely.
Answering Phase: The user’s question and the generated summary are then passed together into a second inference stage.
This approach filters irrelevant noise and allows the LLM to reason over a cleaner, more concise context, improving answer coherence and factual accuracy.
b. Robustness Feature: The Relevance Gate
RAG systems often “hallucinate” when they lack sufficient context. To reduce this, I added a relevance gate that measures how similar the retrieved data is to the query.
If the highest semantic similarity score is below 0.4, the system skips the LLM step and informs the user that it lacks relevant data.
This ensures that every generated response is grounded in retrieved information, making the system more trustworthy and interpretable.
c. Performance Optimization: Asynchronous Retrieval and Caching
To address response latency, I implemented two optimizations:
Asynchronous I/O:
Using Python’s asyncio, the system now retrieves results from Pinecone and Neo4j in parallel instead of sequentially. This cut average response time significantly.
Embedding Caching:
Frequently used embeddings are stored in an in-memory cache, avoiding repeated computation for identical or similar queries during a single session. This optimization lowered response time and computational overhead.

4. Conclusion
This challenge represented a realistic AI engineering scenario, debugging legacy code, adapting to hardware limits, and rethinking architecture under real-world constraints.
The final product is not just a working hybrid AI assistant, but a self-contained RAG pipeline capable of running entirely offline. It integrates optimized inference, robust data retrieval, and practical fail-safes to ensure reliability and efficiency.
Through systematic troubleshooting, hardware-aware optimization, and careful architectural choices, the project achieved its goal: a robust, performant, and cost-efficient AI system built for real-world conditions.
